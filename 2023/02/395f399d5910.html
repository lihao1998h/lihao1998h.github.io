

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="1 我的模块化代码1.1 超参数12345678910import argparsedef parse_args():      parser &#x3D; argparse.ArgumentParser()      parser.add_argument(&amp;#x27;--num-epoch&amp;#x27;, type&#x3D;int, default&#x3D;100, help&#x3D;&amp;#x27;number of traini">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2023/02/395f399d5910.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1 我的模块化代码1.1 超参数12345678910import argparsedef parse_args():      parser &#x3D; argparse.ArgumentParser()      parser.add_argument(&amp;#x27;--num-epoch&amp;#x27;, type&#x3D;int, default&#x3D;100, help&#x3D;&amp;#x27;number of traini">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-02-11T09:45:54.708Z">
<meta property="article:modified_time" content="2023-02-05T11:56:46.000Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text=""></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-02-11 17:45" pubdate>
          2023年2月11日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          48 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none"></h1>
            
            
              <div class="markdown-body">
                
                <h1 id="1-我的模块化代码"><a href="#1-我的模块化代码" class="headerlink" title="1 我的模块化代码"></a>1 我的模块化代码</h1><h2 id="1-1-超参数"><a href="#1-1-超参数" class="headerlink" title="1.1 超参数"></a>1.1 超参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_args</span>():  </span><br><span class="line">    parser = argparse.ArgumentParser()  </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num-epoch&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">100</span>, <span class="built_in">help</span>=<span class="string">&#x27;number of training epochs&#x27;</span>)</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    args = parser.parse_args()  </span><br><span class="line">	<span class="keyword">return</span> args</span><br><span class="line">	</span><br><span class="line">opt = parse_args()</span><br></pre></td></tr></table></figure>
<h2 id="1-2-随机种子设置"><a href="#1-2-随机种子设置" class="headerlink" title="1.2 随机种子设置"></a>1.2 随机种子设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seed_torch</span>(<span class="params">seed=<span class="number">21</span></span>):  </span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)  </span><br><span class="line">    random.seed(seed)  </span><br><span class="line">    np.random.seed(seed)  </span><br><span class="line">    torch.manual_seed(seed)  </span><br><span class="line">    torch.cuda.manual_seed(seed)  </span><br><span class="line">    torch.cuda.manual_seed_all(seed)  </span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span>  </span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">seed_torch(opt.seed)</span><br></pre></td></tr></table></figure>

<h2 id="1-3-cuda"><a href="#1-3-cuda" class="headerlink" title="1.3 cuda"></a>1.3 cuda</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_gpu</span>(<span class="params">x</span>):  </span><br><span class="line">    os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = x  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;using gpu:&#x27;</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定某个GPU</span></span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICE&#x27;</span>]=<span class="string">&#x27;1&#x27;</span></span><br><span class="line">model.cuda()</span><br><span class="line"><span class="comment">#如果是多GPU</span></span><br><span class="line">os.environment[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0,1,2,3&#x27;</span></span><br><span class="line">device_ids = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">net  = torch.nn.Dataparallel(net, device_ids =device_ids)</span><br><span class="line">net  = torch.nn.Dataparallel(net) <span class="comment"># 默认使用所有的device_ids </span></span><br><span class="line">net = net.cuda()</span><br></pre></td></tr></table></figure>

<h2 id="并行训练"><a href="#并行训练" class="headerlink" title="并行训练"></a>并行训练</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/450912044">https://zhuanlan.zhihu.com/p/450912044</a><br>平时一直使用<strong>nn.parallel</strong>，虽然简单只需要一行代码，但是实际的效率却不忍直视，因为每个GPU的负载不均衡，甚至出现多个GPU比单个GPU的训练时间更长的问题，于是决心花时间使用<strong>DistributedDataParallel</strong>进行单机多卡分布式训练，期间遇坑无数，也不是每个问题都能在网上查到，最终花了一天时间才顺利解决，从单卡6个小时一个epoch降低到2块GPU2个小时一个epoch（DistributedDataParallel <strong>yyds</strong>!ヽ(✿ﾟ▽ﾟ)ノ），下面将步骤记录下来并且贴上踩坑记录，希望能帮助更多人。</p>
<h3 id="步骤一：在args里面加上local-rank参数："><a href="#步骤一：在args里面加上local-rank参数：" class="headerlink" title="步骤一：在args里面加上local_rank参数："></a>步骤一：在args里面加上local_rank参数：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">&quot;--local_rank&quot;</span>, default=os.getenv(<span class="string">&#x27;LOCAL_RANK&#x27;</span>, -<span class="number">1</span>), <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure>

<p>这个参数表示前进程对应的GPU号，系统会自动识别，一定要加哦</p>
<h3 id="步骤二：进行初始化："><a href="#步骤二：进行初始化：" class="headerlink" title="步骤二：进行初始化："></a>步骤二：进行初始化：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.local_rank != -<span class="number">1</span>:</span><br><span class="line">    torch.cuda.set_device(args.local_rank)</span><br><span class="line">    device=torch.device(<span class="string">&quot;cuda&quot;</span>, args.local_rank)</span><br><span class="line">    torch.distributed.init_process_group(backend=<span class="string">&quot;nccl&quot;</span>, init_method=<span class="string">&#x27;env://&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>步骤二中我们获得了device用于后续使用</p>
<h3 id="步骤三：将创建好的模型放在GPU上，并且使用DistributedDataParallel"><a href="#步骤三：将创建好的模型放在GPU上，并且使用DistributedDataParallel" class="headerlink" title="步骤三：将创建好的模型放在GPU上，并且使用DistributedDataParallel"></a>步骤三：将创建好的模型放在GPU上，并且使用DistributedDataParallel</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)</span><br><span class="line">num_gpus = torch.cuda.device_count()</span><br><span class="line"><span class="keyword">if</span> num_gpus &gt; <span class="number">1</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;use &#123;&#125; gpus!&#x27;</span>.<span class="built_in">format</span>(num_gpus))</span><br><span class="line">    model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],</span><br><span class="line">                                                output_device=args.local_rank)</span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<p>记住要先放在device上再进行DistributedDataParallel哦，不然会报错</p>
<h3 id="步骤四：改动数集dataloader的输入"><a href="#步骤四：改动数集dataloader的输入" class="headerlink" title="步骤四：改动数集dataloader的输入"></a><strong>步骤四：改动数集dataloader的输入</strong></h3><p>在定义dataloader的时候将数据集的sampler从RandomSampler改成DistributedSampler哦，然后dataloader的pin_memory设置为True就行啦！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line">train_datasets = ...<span class="comment">#自己定义的Dataset子类</span></span><br><span class="line">train_sampler = DistributedSampler(train_datasets)</span><br><span class="line">train_dataloader = DataLoader(train_datasets, sampler=train_sampler, batch_size=args.train_batch_size,</span><br><span class="line">                                  num_workers=args.num_workers, pin_memory=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>到这里py文件里面的代码就基本上ok啦，后面的代码和 nn.parallel 一样正常使用不需要改变</p>
<h3 id="步骤五：在dataloader中加入打乱顺序（shuffle）的操作（可加）"><a href="#步骤五：在dataloader中加入打乱顺序（shuffle）的操作（可加）" class="headerlink" title="步骤五：在dataloader中加入打乱顺序（shuffle）的操作（可加）"></a>步骤五：在dataloader中加入打乱顺序（<strong>shuffle</strong>）的操作（可加）</h3><p>这个也是参考别人的，在分布式模式下，需要在每个 epoch 开始时调用set_epoch()方法，然后再创建 DataLoader 迭代器，以使shuffle 操作能够在多个 epoch 中正常工作。 否则，dataloader迭代器产生的数据将始终使用相同的顺序，使得每个epoch在每个GPU上分割的数据集都是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">        train_sampler.set_epoch(epoch) <span class="comment">#shuffle</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader :</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure>

<h3 id="步骤六：启动程序"><a href="#步骤六：启动程序" class="headerlink" title="步骤六：启动程序"></a>步骤六：启动程序</h3><p>启动程序的指令和一般情况的不一样，例如启动main.py文件</p>
<p>一般情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py</span><br></pre></td></tr></table></figure>

<p>分布式启动需要加额外指令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=<span class="number">2</span> main.py</span><br></pre></td></tr></table></figure>

<p>这里“nproc_per_node=2”是指我这个节点使用2块GPU</p>
<p>到此运行程序就可以了</p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> (DataLoader, Dataset)</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment">#步骤一：定义local_rank</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    ...</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--local_rank&quot;</span>, default=os.getenv(<span class="string">&#x27;LOCAL_RANK&#x27;</span>, -<span class="number">1</span>), <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#步骤二：初始化</span></span><br><span class="line">    <span class="keyword">if</span> args.local_rank != -<span class="number">1</span>:</span><br><span class="line">        torch.cuda.set_device(args.local_rank)</span><br><span class="line">        device=torch.device(<span class="string">&quot;cuda&quot;</span>, args.local_rank)</span><br><span class="line">        torch.distributed.init_process_group(backend=<span class="string">&quot;nccl&quot;</span>, init_method=<span class="string">&#x27;env://&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#步骤三：模型分布式处理</span></span><br><span class="line">    model = MODEL()<span class="comment">#创建模型</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    num_gpus = torch.cuda.device_count()</span><br><span class="line">    <span class="keyword">if</span> num_gpus &gt; <span class="number">1</span>:</span><br><span class="line">        logger.info(<span class="string">&#x27;use &#123;&#125; gpus!&#x27;</span>.<span class="built_in">format</span>(num_gpus))</span><br><span class="line">        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],</span><br><span class="line">                                                output_device=args.local_rank)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#步骤四：定义数据集</span></span><br><span class="line">    train_datasets = ...<span class="comment">#自己定义的Dataset子类</span></span><br><span class="line">    train_sampler = DistributedSampler(train_datasets)</span><br><span class="line">    train_dataloader = DataLoader(train_datasets, sampler=train_sampler, batch_size=args.train_batch_size,</span><br><span class="line">                                  num_workers=args.num_workers, pin_memory=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">         <span class="comment">#步骤五：打乱顺序</span></span><br><span class="line">        train_sampler.set_epoch(epoch)</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader :</span><br><span class="line">            loss = model(batch.cuda())</span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h3 id="最后，运行中可能出现的问题："><a href="#最后，运行中可能出现的问题：" class="headerlink" title="最后，运行中可能出现的问题："></a>最后，运行中可能出现的问题：</h3><p>**1.**如果出现GPU忙碌或者无法获得：CUDA error: all CUDA-capable devices are busy or unavailable：需要获得步骤二的device并且在步骤三中将model放在device上就行</p>
<p>**2.**torch.distributed.init_process_group(backend=”nccl”, init_method=’env://‘) 出现错误 Address already in use，告知地址无效或者地址被占用：这里只需要在启动命令加一个指令 <strong>–master_port 29501</strong>即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=<span class="number">2</span> --master_port <span class="number">29501</span> main.py</span><br></pre></td></tr></table></figure>

<p><strong>3.<strong>如果出现inplace操作问题，如：one of the variables needed for gradient computation has been modified by an inplace operation，并且出现在batchnorm方法中(</strong><em>如果不知道错误在哪，可以往下看</em><strong>），只需要将DistributedDataParallel的</strong>broadcast_buffers</strong>参数改为<strong>False</strong>即可，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)</span><br><span class="line">num_gpus = torch.cuda.device_count()</span><br><span class="line"><span class="keyword">if</span> num_gpus &gt; <span class="number">1</span>:</span><br><span class="line">    logger.info(<span class="string">&#x27;use &#123;&#125; gpus!&#x27;</span>.<span class="built_in">format</span>(num_gpus))</span><br><span class="line">    model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],</span><br><span class="line">                                                output_device=args.local_rank, broadcast_buffers=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>如果不是batchnorm出现的，看看代码中是否有x+=y或者Relu(inpalce=True)的操作，需要改成x=x+y以及Relu(inpalce=False)</p>
<p>或者使用**torch.autograd.set_detect_anomaly(True)**放在前向传播代码前（即训练模型的代码前），运行程序的时候遇到错误会自动打印详细的错误路径，方便查找哦</p>
<p>**4.**保存模型的时候，在每一层的名字中， 多卡会比单卡 多一个“module”， 所以保存的是model.module而不是model。即torch.save(model.module, “模型名字”)就行啦。当然保存model也可以，只要下次加载也是在多卡上就行。</p>
<p><strong>5.<strong>模型在 nn.parallel.DistributedDataParallel出现“</strong>Socket Timeout</strong>”问题 ，评论区的同学通过升级pytorch1.7以上的版本成功运行了，<a href="https://link.zhihu.com/?target=https://github.com/pytorch/pytorch/issues/25767">https://github.com/pytorch/pytorch/issues/25767</a> 也有人表示升级NCCL版本。这个问题可能是版本的问题导致了通信错误。</p>
<ol start="6">
<li>若出现<strong>RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.</strong>…则<br>在DistributedDataParallel 中加入find_unused_parameters=True</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div></div>
      <div>http://example.com/2023/02/395f399d5910.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年2月11日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/02/cc236feb250b.html" title="">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile"></span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/02/80fa6397f621.html" title="">
                        <span class="hidden-mobile"></span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
