<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="相关领域[[关键点检测]] 子领域[[行人检测]] 其它概念Anchor Box![[Anchor box.png]]  一个对象分配到多个Anchor Box 处理对象出现在同一个格子里的情况  分支领域姿势估计Pose Estimation Metrics Object Keypoint Similarity (OKS)   SOTA HRNET Sun, K. , Xiao, B. , Li">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2023/02/ef02a4c91f4c.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="相关领域[[关键点检测]] 子领域[[行人检测]] 其它概念Anchor Box![[Anchor box.png]]  一个对象分配到多个Anchor Box 处理对象出现在同一个格子里的情况  分支领域姿势估计Pose Estimation Metrics Object Keypoint Similarity (OKS)   SOTA HRNET Sun, K. , Xiao, B. , Li">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-02-11T09:45:54.838Z">
<meta property="article:modified_time" content="2023-02-05T11:56:46.000Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/fluid.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-计算机视觉/目标检测/目标检测" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/02/ef02a4c91f4c.html" class="article-date">
  <time datetime="2023-02-11T09:45:54.838Z" itemprop="datePublished">2023-02-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
  </div>

  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="相关领域"><a href="#相关领域" class="headerlink" title="相关领域"></a>相关领域</h1><p>[[关键点检测]]</p>
<h1 id="子领域"><a href="#子领域" class="headerlink" title="子领域"></a>子领域</h1><p>[[行人检测]]</p>
<h2 id="其它概念"><a href="#其它概念" class="headerlink" title="其它概念"></a>其它概念</h2><h3 id="Anchor-Box"><a href="#Anchor-Box" class="headerlink" title="Anchor Box"></a>Anchor Box</h3><p>![[Anchor box.png]]</p>
<ul>
<li>一个对象分配到多个Anchor Box</li>
<li>处理对象出现在同一个格子里的情况</li>
</ul>
<h2 id="分支领域"><a href="#分支领域" class="headerlink" title="分支领域"></a>分支领域</h2><h3 id="姿势估计Pose-Estimation"><a href="#姿势估计Pose-Estimation" class="headerlink" title="姿势估计Pose Estimation"></a>姿势估计Pose Estimation</h3><ul>
<li>Metrics<ul>
<li>Object Keypoint Similarity (OKS)</li>
</ul>
</li>
<li>SOTA<ul>
<li>HRNET<ul>
<li>Sun, K. , Xiao, B. , Liu, D. , and Wang, J: Deep High-Resolution Representation Learning for Human Pose Estimation, in CVPR, 2019.</li>
</ul>
</li>
<li>EMpose<ul>
<li>Li X, Zhong Z, Wu J, et al: Expectation-maximization attention networks for semantic segmentation, in ICCV, 2019.</li>
</ul>
</li>
<li>CIposeNet<ul>
<li>HRNET+因果干预</li>
<li>ACPR’21</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="行人检测"><a href="#行人检测" class="headerlink" title="行人检测"></a>行人检测</h3><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="MSCOCO"><a href="#MSCOCO" class="headerlink" title="MSCOCO"></a>MSCOCO</h3><ul>
<li>包含超过200000张图片以及超过250000个人类实例</li>
<li>3种标注类型<ul>
<li>object instances（目标实例）,</li>
<li>object keypoints（目标上的关键点）,</li>
<li>和image captions（看图说话）</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29393415">https://zhuanlan.zhihu.com/p/29393415</a></li>
</ul>
</li>
<li>Lin T Y, Maire M, Belongie S, et al: Microsoft coco: Common objects in context, in CVPR, 2014.</li>
<li>2017<ul>
<li>COCO train2017<ul>
<li>57K images</li>
<li>150K person instances<br>  ![[Pasted image 20221127223044.png]]</li>
</ul>
</li>
<li>val2017<ul>
<li>5000 images<br>  ![[Pasted image 20221127223051.png]]</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012505617/article/details/106517073/">https://blog.csdn.net/u012505617/article/details/106517073/</a></li>
</ul>
</li>
<li>COCO处理<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/donkey_1993/article/details/106279988">https://blog.csdn.net/donkey_1993/article/details/106279988</a></li>
</ul>
</li>
<li>annotation<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29393415">https://zhuanlan.zhihu.com/p/29393415</a></li>
</ul>
</li>
</ul>
<h3 id="the-PASCAL-VOC-data-sets"><a href="#the-PASCAL-VOC-data-sets" class="headerlink" title="the PASCAL VOC data sets"></a>the PASCAL VOC data sets</h3><ul>
<li>M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes (VOC) Challenge,” Int’l J. Computer Vision, vol. 88, no. 2, pp. 303-338, June 2010.</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/mzpmzk/article/details/88065416">https://blog.csdn.net/mzpmzk/article/details/88065416</a></li>
<li>1.VOC2007 总共：9963 test： 4952 trainval：5011（train： 2501 ; val： 2510）</li>
</ul>
<p>2.VOC2012 trainval： 11540 ( train：5717 ; val：5823) 只是有11540张用于检测任务，下载VOC数据集中JPEGImages文件夹存储了17125张图片，没有全部用到检测</p>
<ul>
<li>数据下载<ul>
<li><a target="_blank" rel="noopener" href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">https://pjreddie.com/projects/pascal-voc-dataset-mirror/</a></li>
</ul>
</li>
</ul>
<h3 id="OpenImages"><a href="#OpenImages" class="headerlink" title="OpenImages"></a>OpenImages</h3><ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010167269/article/details/52717394/">https://blog.csdn.net/u010167269/article/details/52717394/</a></li>
<li><a target="_blank" rel="noopener" href="https://research.googleblog.com/2016/09/introducing-open-images-dataset.html">https://research.googleblog.com/2016/09/introducing-open-images-dataset.html</a></li>
</ul>
<h2 id="相关库"><a href="#相关库" class="headerlink" title="相关库"></a>相关库</h2><h3 id="MMDetection"><a href="#MMDetection" class="headerlink" title="MMDetection"></a>MMDetection</h3><h3 id="https-github-com-tensorflow-models-tree-master-research-object-detection"><a href="#https-github-com-tensorflow-models-tree-master-research-object-detection" class="headerlink" title="https://github.com/tensorflow/models/tree/master/research/object_detection"></a><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/research/object_detection">https://github.com/tensorflow/models/tree/master/research/object_detection</a></h3><h2 id="物体位置检测"><a href="#物体位置检测" class="headerlink" title="物体位置检测"></a>物体位置检测</h2><h3 id="检测任务包含两个子任务，其一是这一目标的类别信息和概率，它是一个分类任务。其二是目标的具体位置信息，这是一个定位任务。"><a href="#检测任务包含两个子任务，其一是这一目标的类别信息和概率，它是一个分类任务。其二是目标的具体位置信息，这是一个定位任务。" class="headerlink" title="检测任务包含两个子任务，其一是这一目标的类别信息和概率，它是一个分类任务。其二是目标的具体位置信息，这是一个定位任务。"></a>检测任务包含两个子任务，其一是这一目标的类别信息和概率，它是一个分类任务。其二是目标的具体位置信息，这是一个定位任务。</h3><p>![[Pasted image 20221127223318.png]]</p>
<h3 id="输出y"><a href="#输出y" class="headerlink" title="输出y"></a>输出y</h3><ul>
<li>bounding box<ul>
<li>[bx,by,bh,bw]</li>
<li>bx,by对应中心点坐标<ul>
<li>左上角为（0,0）</li>
</ul>
</li>
<li>bh，bw<ul>
<li>高和宽</li>
</ul>
</li>
</ul>
</li>
<li>[c1,c2,c3,…]<ul>
<li>分类信息</li>
</ul>
</li>
<li>pc<ul>
<li>图像中有物体的概率</li>
</ul>
</li>
</ul>
<h2 id="分类及算法"><a href="#分类及算法" class="headerlink" title="分类及算法"></a>分类及算法</h2><h3 id="传统算法"><a href="#传统算法" class="headerlink" title="传统算法"></a>传统算法</h3><ul>
<li>暴力检测/滑动窗口<ul>
<li>for window in windows: patchs = get_patch(image, window) results = detector(patchs)</li>
<li>从左到右，从上到下</li>
<li>计算量太大</li>
</ul>
</li>
<li>Viola Jones检测器<ul>
<li>18年前，P. Viola和M. Jones在没有任何约束(如肤色分割)的情况下首次实现了人脸的实时检测<a href="#">8</a>。他们所设计的检测器在一台配备700MHz Pentium III CPU的电脑上运行，在保持同等检测精度的条件下的运算速度是其他算法的数十甚至数百倍。这种检测算法以共同作者的名字命名为“Viola-Jones (VJ) 检测器”以纪念他们的重大贡献。</li>
<li>VJ检测器采用最直接的检测方法，即滑动窗口(slide window)：查看一张图像中所有可能的窗口尺寸和位置并判断是否有窗口包含人脸。这一过程虽然听上去简单，但它背后所需的计算量远远超出了当时计算机的算力。VJ检测器结合了 “ 积分图像 ”、“ 特征选择 ” 和 “ 检测级联 ” 三种重要技术，大大提高了检测速度。</li>
<li>1）积分图像：这是一种计算方法，以加快盒滤波或卷积过程。与当时的其他目标检测算法一样[10]，在VJ检测器中使用Haar小波作为图像的特征表示。积分图像使得VJ检测器中每个窗口的计算复杂度与其窗口大小无关。</li>
<li>2）特征选择：作者没有使用一组手动选择的Haar基过滤器，而是使用Adaboost算法从一组巨大的随机特征池 (大约18万维) 中选择一组对人脸检测最有帮助的小特征。</li>
<li>3）检测级联：在VJ检测器中引入了一个多级检测范例 ( 又称“检测级联”，detection cascades )，通过减少对背景窗口的计算，而增加对人脸目标的计算，从而减少了计算开销。</li>
</ul>
</li>
<li>HOG 检测器  DalalN, Triggs B. Histograms of Oriented Gradients for Human Detection [C].IEEEComputer Society Conference on Computer Vision &amp; Pattern Recognition.IEEEComputer Society, 2005:886-893. 3</li>
<li>Harr  P.Viola and M.Jones. Rapid object detection using a boosted cascade of simple features. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, 2001</li>
<li>基于可变形部件的模型(DPM)<ul>
<li>DPM作为voco -07、-08、-09届检测挑战赛的优胜者，它曾是传统目标检测方法的巅峰。DPM最初是由P. Felzenszwalb提出的[12]，于2008年作为HOG检测器的扩展，之后R. Girshick进行了各种改进<a href="#">13</a>。<ul>
<li>P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645, 2010</li>
</ul>
</li>
<li>DPM遵循“分而治之”的检测思想，训练可以简单地看作是学习一种正确的分解对象的方法，推理可以看作是对不同对象部件的检测的集合。例如，检测“汽车”的问题可以看作是检测它的窗口、车身和车轮。工作的这一部分，也就是“star model”由P.Felzenszwalb等人完成。后来，R. Girshick进一步将star model扩展到 “ 混合模型 ”，以处理更显著变化下的现实世界中的物体。</li>
<li>一个典型的DPM检测器由一个根过滤器（root-filter）和一些零件滤波器（part-filters）组成。该方法不需要手动设定零件滤波器的配置（如尺寸和位置），而是在开发了一种弱监督学习方法并使用到了DPM中，所有零件滤波器的配置都可以作为潜在变量自动学习。R. Girshick将这个过程进一步表述为一个多实例学习的特殊案例，同时还应用了“困难负样本挖掘（hard-negative mining）”、“边界框回归”、“语境启动”等重要技术以提高检测精度。而为了加快检测速度，Girshick开发了一种技术，将检测模型“ 编译 ”成一个更快的模型，实现了级联结构，在不牺牲任何精度的情况下实现了超过10倍的加速。</li>
<li>虽然今天的目标探测器在检测精度方面已经远远超过了DPM，但仍然受到DPM的许多有价值的见解的影响，如混合模型、困难负样本挖掘、边界框回归等。2010年，P. Felzenszwalb和R. Girshick被授予PASCAL VOC的 “终身成就奖”。</li>
</ul>
</li>
</ul>
<h3 id="非极大值抑制（Non-max-suppression-algorithm-算法"><a href="#非极大值抑制（Non-max-suppression-algorithm-算法" class="headerlink" title="非极大值抑制（Non-max suppression algorithm)算法"></a>非极大值抑制（Non-max suppression algorithm)算法</h3><ul>
<li>首先选择最大概率为目标的框，让它高亮，并删除和它高交并比的框，重复选择和删除即可。</li>
<li>直到所有边界框被选择</li>
<li>用于筛选边界框</li>
</ul>
<h3 id="子主题-5"><a href="#子主题-5" class="headerlink" title="子主题 5"></a>子主题 5</h3><ul>
<li>RPN (NeurIPS’2015)</li>
<li>Fast R-CNN (ICCV’2015)</li>
<li>Faster R-CNN (NeurIPS’2015)</li>
<li>Mask R-CNN (ICCV’2017)</li>
<li>Cascade R-CNN (CVPR’2018)</li>
<li>Cascade Mask R-CNN (CVPR’2018)</li>
<li>SSD (ECCV’2016)</li>
<li>RetinaNet (ICCV’2017)</li>
<li>GHM (AAAI’2019)</li>
<li>Mask Scoring R-CNN (CVPR’2019)</li>
<li>Double-Head R-CNN (CVPR’2020)</li>
<li>Hybrid Task Cascade (CVPR’2019)</li>
<li>Libra R-CNN (CVPR’2019)</li>
<li>Guided Anchoring (CVPR’2019)</li>
<li>FCOS (ICCV’2019)</li>
<li>RepPoints (ICCV’2019)</li>
<li>Foveabox (TIP’2020)</li>
<li>FreeAnchor (NeurIPS’2019)</li>
<li>NAS-FPN (CVPR’2019)</li>
<li>ATSS (CVPR’2020)</li>
<li>FSAF (CVPR’2019)</li>
<li>PAFPN (CVPR’2018)</li>
<li>Dynamic R-CNN (ECCV’2020)</li>
<li>PointRend (CVPR’2020)</li>
<li>CARAFE (ICCV’2019)</li>
<li>DCNv2 (CVPR’2019)</li>
<li>Group Normalization (ECCV’2018)</li>
<li>Weight Standardization (ArXiv’2019)</li>
<li>OHEM (CVPR’2016)</li>
<li>Soft-NMS (ICCV’2017)</li>
<li>Generalized Attention (ICCV’2019)</li>
<li>GCNet (ICCVW’2019)</li>
<li>Mixed Precision (FP16) Training (ArXiv’2017)</li>
<li>InstaBoost (ICCV’2019)</li>
<li>GRoIE (ICPR’2020)</li>
<li>DetectoRS (ArXiv’2020)</li>
<li>Generalized Focal Loss (NeurIPS’2020)</li>
<li>CornerNet (ECCV’2018)</li>
<li>Side-Aware Boundary Localization (ECCV’2020)</li>
<li>YOLOv3 (ArXiv’2018)</li>
<li>PAA (ECCV’2020)</li>
<li>YOLACT (ICCV’2019)</li>
<li>CentripetalNet (CVPR’2020)</li>
<li>VFNet (ArXiv’2020)</li>
<li>DETR (ECCV’2020)</li>
<li>Deformable DETR (ICLR’2021)</li>
<li>CascadeRPN (NeurIPS’2019)</li>
<li>SCNet (AAAI’2021)</li>
<li>AutoAssign (ArXiv’2020)</li>
<li>YOLOF (CVPR’2021)</li>
<li>Seasaw Loss (CVPR’2021)</li>
<li>CenterNet (CVPR’2019)</li>
<li>YOLOX (ArXiv’2021)</li>
<li>SOLO (ECCV’2020)</li>
</ul>
<h2 id="难题"><a href="#难题" class="headerlink" title="难题"></a>难题</h2><h3 id="目标检测方向有一些固有的难题，比如小脸，遮挡，大姿态。"><a href="#目标检测方向有一些固有的难题，比如小脸，遮挡，大姿态。" class="headerlink" title="目标检测方向有一些固有的难题，比如小脸，遮挡，大姿态。"></a>目标检测方向有一些固有的难题，比如小脸，遮挡，大姿态。</h3><h3 id="而在方法上，多尺度与级联网络的设计，难样本的挖掘，多任务loss等都是比较大的研究小方向"><a href="#而在方法上，多尺度与级联网络的设计，难样本的挖掘，多任务loss等都是比较大的研究小方向" class="headerlink" title="而在方法上，多尺度与级联网络的设计，难样本的挖掘，多任务loss等都是比较大的研究小方向"></a>而在方法上，多尺度与级联网络的设计，难样本的挖掘，多任务loss等都是比较大的研究小方向</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/02/ef02a4c91f4c.html" data-id="cldzsk2aj002yd8nc0w98eac7" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/02/e45d12b1196a.html" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2023/02/4a85df63b553.html" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/02/47c93aa5802a.html">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/02/57a6e32ff281.html">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/02/e306f9f8860c.html">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/02/d42e9f60b4b0.html">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/02/7411c7521421.html">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>